{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f710be4e",
   "metadata": {
    "id": "f710be4e"
   },
   "source": [
    "# Premières estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e7d4f9",
   "metadata": {
    "id": "69e7d4f9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from numpy import sqrt\n",
    "from numpy import logspace\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91548e9c",
   "metadata": {
    "id": "91548e9c"
   },
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e890d86c",
   "metadata": {
    "id": "e890d86c"
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(r\"C:\\Users\\Tempo\\Travilage\\Satges\\2A_Datacraft\\Technik\\content\\forecasting.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fa02ce",
   "metadata": {
    "id": "b6fa02ce"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30898669",
   "metadata": {
    "id": "30898669"
   },
   "outputs": [],
   "source": [
    "df.fillna(method ='pad',inplace = True)\n",
    "df.drop(['location_index','customer_index'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c680dc0",
   "metadata": {
    "id": "0c680dc0"
   },
   "outputs": [],
   "source": [
    "min_max_scaler=MinMaxScaler()\n",
    "l_df=[]#list of the whole dataframe grouped per product\n",
    "l_df_without_lags=[]#we remove lags\n",
    "l_df_without_lags_scaled=[]#we remove lags and rescale\n",
    "number=[]\n",
    "products=pd.unique(df['product_index'])\n",
    "for index in range(len(products)):\n",
    "  df_temp=df[df['product_index']==products[index]].sort_values(by=\"time_index\")\n",
    "  l_df.append(df_temp)\n",
    "  number.append(df_temp.shape)\n",
    "  df_temp_without_lags=pd.DataFrame({'time_index' : df_temp['time_index'],'apparenttemperaturemax':df_temp['apparenttemperaturemax_minus_1'],\n",
    "                         'cos_iso_month':df_temp['cos_iso_month'],\n",
    " 'cos_iso_week':df_temp['cos_iso_week'],\n",
    " 'cos_iso_week_of_month':df_temp['cos_iso_week_of_month'], 'days_before_next_holiday'\n",
    " :df_temp['days_before_next_holiday'],'forecasted_volumes' :df_temp['forecasted_volumes'],'holiday_day_of_week':df_temp['holiday_day_of_week'],\n",
    " 'holidays_count_in_week':df_temp['holidays_count_in_week'],\n",
    " 'iso_month':df_temp['iso_month'],\n",
    " 'iso_week':df_temp['iso_week'],'promo_mean_horizon_14':df_temp['promo_mean_horizon_14'],\n",
    " 'iso_week_of_month':df_temp['iso_week_of_month'],\n",
    " 'mat_net_weight_value_kg':df_temp['mat_net_weight_value_kg'],\n",
    " 'ordered_volumes':df_temp['ordered_volumes'],'precipintensity':df_temp['precipintensity'],'promo_uplift_coefficient':df_temp['promo_uplift_coefficient']})\n",
    "  l_df_without_lags.append(df_temp_without_lags)\n",
    "  df_temp_without_lags_rescaled=df_temp_without_lags.copy(deep=True)\n",
    "  df_temp_without_lags_rescaled[['forecasted_volumes','ordered_volumes']]=min_max_scaler.fit_transform(df_temp_without_lags[['forecasted_volumes','ordered_volumes']])\n",
    "  l_df_without_lags_scaled.append(df_temp_without_lags_rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91515985",
   "metadata": {
    "id": "91515985",
    "outputId": "eae68b2b-384c-4065-a4f6-5fcaae311da6"
   },
   "outputs": [],
   "source": [
    "l_df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c72d81f",
   "metadata": {
    "id": "6c72d81f"
   },
   "source": [
    "### Selection arbitraire de produits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2178ee",
   "metadata": {
    "id": "2e2178ee",
    "outputId": "61596581-4912-40b7-cfc0-d488a730be40"
   },
   "outputs": [],
   "source": [
    "P1 = l_df_without_lags[0].set_index(\"time_index\").drop([\"forecasted_volumes\", \"mat_net_weight_value_kg\"], axis=1)\n",
    "P1sc = l_df_without_lags_scaled[0].set_index(\"time_index\").drop([\"forecasted_volumes\"], axis=1)\n",
    "P2 = l_df_without_lags[1].set_index(\"time_index\").drop([\"forecasted_volumes\"], axis=1)\n",
    "P3 = l_df_without_lags[14].set_index(\"time_index\").drop([\"forecasted_volumes\"], axis=1)\n",
    "P4 = l_df_without_lags[15].set_index(\"time_index\").drop([\"forecasted_volumes\"], axis=1)\n",
    "P5 = l_df_without_lags[123].set_index(\"time_index\").drop([\"forecasted_volumes\"], axis=1)\n",
    "\n",
    "T = pd.to_datetime(l_df_without_lags[0][\"time_index\"])\n",
    "\n",
    "Produits = [P1, P2, P3, P4, P5]\n",
    "for i in range(5):\n",
    "    print(\"longueur de P\" + str(i+1) +\":\" ,len(Produits[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b49101e",
   "metadata": {
    "id": "4b49101e",
    "outputId": "bddcc1bf-6297-4ef9-fcf0-abccf013a112",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "P1['ordered_volumes'].loc[:'20211230'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895bb50d",
   "metadata": {
    "id": "895bb50d",
    "outputId": "0a73e163-13b2-48cf-aca9-2265d4155e65"
   },
   "outputs": [],
   "source": [
    "P1['ordered_volumes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3717a27",
   "metadata": {
    "id": "f3717a27"
   },
   "source": [
    "### Des tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050960ae",
   "metadata": {
    "id": "050960ae",
    "outputId": "b68ec434-c6ad-4b6c-9d1f-eba2440bcb92"
   },
   "outputs": [],
   "source": [
    "list(P1.drop([\"ordered_volumes\"], axis=1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8ee87d",
   "metadata": {
    "id": "ec8ee87d",
    "outputId": "402a5599-ffa6-40e9-8dc2-df2662180e57",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "D = P1.set_index(\"time_index\")\n",
    "X = D.drop([\"ordered_volumes\", \"mat_net_weight_value_kg\"], axis=1)\n",
    "Y = pd.DataFrame(list(D[\"ordered_volumes\"]), index= pd.to_datetime(P1[\"time_index\"]))\n",
    "\n",
    "\n",
    "def movAvg(Y,n):\n",
    "    return Y.rolling(n).mean().dropna()\n",
    "\n",
    "mY = movAvg(Y, 40)\n",
    "pd.DataFrame(pacf(mY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb8d687",
   "metadata": {
    "id": "0cb8d687",
    "outputId": "69beacc1-4115-4d45-957c-b36d970c8c29",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=10, ncols=4, figsize=(16, 10))\n",
    "\n",
    "for i in range(40):\n",
    "    plt.subplot(10, 4, i+1)\n",
    "    mY = movAvg(Y, i+1)\n",
    "    sns.lineplot(x = 'time_index', y = 0,data = mY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dc6972",
   "metadata": {
    "id": "a4dc6972",
    "outputId": "c14c5400-30df-473d-be2d-397c44ef3f47",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=10, ncols=4, figsize=(16, 10))\n",
    "\n",
    "for i in range(40):\n",
    "    plt.subplot(10, 4, i+1)\n",
    "    mY = movAvg(Y, i+1)\n",
    "    acfdf = pd.DataFrame(pacf(mY))\n",
    "    acfdf.plot(ax=axes[i//4,i%4], kind='bar' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c37101",
   "metadata": {
    "id": "65c37101",
    "outputId": "7d5d9186-d36f-4bbe-e44f-a335c5537238"
   },
   "outputs": [],
   "source": [
    "mY = movAvg(Y, 3)\n",
    "numY = mY.to_numpy()\n",
    "\n",
    "s = 0\n",
    "for i in range(len(mY)-40):\n",
    "    s += numY[i+4] - numY[i+3]\n",
    "s/(len(mY)-40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5449cb",
   "metadata": {
    "id": "be5449cb"
   },
   "source": [
    "On considère un modèle AR(1) pour $Z(t) = \\frac{1}{3}\\sum_{i=0}^{2}Y(t-i)$:\n",
    "\n",
    "$Z(t) = c + \\phi Z(t-1) + \\epsilon_{t}$\n",
    "\n",
    "Il s'ensuit:\n",
    "\n",
    "$Y(t) = 3c + (\\phi - 1)(Y(t-1) + Y(t-2)) + \\phi Y(t-3) + 3\\epsilon_{t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290807f8",
   "metadata": {
    "id": "290807f8",
    "outputId": "1d7c4b5d-99f6-492f-cd53-fba7205b3999",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y.to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b822a82a",
   "metadata": {
    "id": "b822a82a"
   },
   "outputs": [],
   "source": [
    "c = -0.12\n",
    "phi = acf(mY)[1]\n",
    "Z_ = mY.to_numpy()\n",
    "Y_num = Y.to_numpy().ravel()\n",
    "Y_h = []\n",
    "for i in range(2,len(Y)-10):\n",
    "    Y_h.append(float(3*Z_[i-1]-(Y_num[i-1]+Y_num[i-2])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e720eae2",
   "metadata": {
    "id": "e720eae2",
    "outputId": "73fbab9e-082b-4517-af72-21442f9cde82"
   },
   "outputs": [],
   "source": [
    "T[:len(T)-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a2684a",
   "metadata": {
    "id": "69a2684a",
    "outputId": "cb4a4e97-405d-4146-c921-89e93e20c232",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = {'Y':Y_num.tolist()[2:len(Y)-10], 'Y estimé': Y_h}\n",
    "pd.DataFrame(data, index = T[2:len(Y)-10]).plot(figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ff0acd",
   "metadata": {
    "id": "69ff0acd",
    "outputId": "1756e48b-e91a-4876-8699-1dfc362dd6ce",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=32, ncols=4, figsize=(16 ,30))\n",
    "\n",
    "for i in range(125):\n",
    "    D = l_df_without_lags[i].set_index(\"time_index\")\n",
    "    Y = pd.DataFrame(list(D[\"ordered_volumes\"]), index= pd.to_datetime(l_df_without_lags[i][\"time_index\"]))\n",
    "    plt.subplot(32, 4, i+1)\n",
    "    mY = movAvg(Y, 52)\n",
    "    sns.lineplot(x = 'time_index', y = 'AVG',data = mY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e86d11",
   "metadata": {
    "id": "29e86d11"
   },
   "source": [
    "### Un peu plus de preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1ebcd0",
   "metadata": {
    "id": "9d1ebcd0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(Data, ratio_train_test = 0.8, n_test = None):\n",
    "    if n_test == None:\n",
    "        n_test = int(len(Data)*ratio_train_test)\n",
    "    if(n_test < len(Data)):\n",
    "        return Data[:len(Data) - n_test], Data[len(Data) - n_test:]\n",
    "    print('nombre de données test trop grand')\n",
    "    return None, Data\n",
    "\n",
    "def movAvg(Y,n):\n",
    "    return Y.rolling(n).mean().dropna()\n",
    "\n",
    "def lagDf(Data, n):\n",
    "    cols = []\n",
    "    for i in range(n+1):\n",
    "        cols.append(Data.shift(7*i, 'D'))\n",
    "    return pd.concat(cols, axis = 1)\n",
    "    \n",
    "\n",
    "def preprocess(Data, method = '', ratio_train_test = 0.8, n_test = None):\n",
    "    X = Data.drop([\"ordered_volumes\", \"mat_net_weight_value_kg\"], axis=1)\n",
    "    Y = Data[\"ordered_volumes\"]\n",
    "    \n",
    "    if (method =='stationnarisation'):\n",
    "        Y = Y.to_frame()\n",
    "        Y_avg = movAvg(Y, 4)\n",
    "        A = Y_avg*(-1)\n",
    "        Y_bis = pd.concat([Y,A], axis=1).sum(axis=1).to_frame()\n",
    "        Y_bis['lag'] = Y_bis.shift(7*(52), 'D')*-1\n",
    "        Y_bis['lag'].fillna(Y_bis['lag'].mean(), inplace=True)\n",
    "        Y = Y_bis.sum(axis=1).loc['2017-01-01':'2022-05-17']\n",
    "        method = 'lags'\n",
    "    \n",
    "    if (method == 'lags'):\n",
    "        X_lags = []\n",
    "        X_tobelagged = X.drop([\"cos_iso_month\", \"cos_iso_week\", \"cos_iso_week_of_month\", \"days_before_next_holiday\", \"holiday_day_of_week\", \"holidays_count_in_week\", \"iso_month\", 'iso_week', 'iso_week_of_month'], axis=1)\n",
    "        X_tobelagged = pd.concat([X_tobelagged, Y], axis = 1)\n",
    "        for i in range(54):\n",
    "            X_lags.append(X_tobelagged.shift(7*(i+1), 'D'))\n",
    "        X = pd.concat([X] + X_lags, axis = 1).loc['20180115':Data.index[-1]]\n",
    "        Y = Y.loc['20180115': Data.index[-1]]\n",
    "    \n",
    "\n",
    "    \n",
    "    return train_test_split(X, ratio_train_test, n_test) + train_test_split(Y, ratio_train_test, n_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380be62b",
   "metadata": {
    "id": "380be62b",
    "outputId": "0c40dc43-6677-4811-eb39-2f8b9db99f53"
   },
   "outputs": [],
   "source": [
    "train_lag = lagDf(Y_train, 12).dropna()\n",
    "X, Y = train_lag.iloc[:,1:], train_lag.iloc[:,0]\n",
    "history = [x for x in Y_train] \n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3392ac",
   "metadata": {
    "id": "ce3392ac",
    "outputId": "7e4dfc18-eb06-4459-bbd0-3a9d0ae6fdd2"
   },
   "outputs": [],
   "source": [
    "(X_train, X_test, Y_train, Y_test) = preprocess(P1, method = 'stationnarisation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb80af3",
   "metadata": {
    "id": "6cb80af3"
   },
   "outputs": [],
   "source": [
    "(X_train, X_test, Y_train, Y_test) = preprocess(P1, method='lags')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e6345",
   "metadata": {
    "id": "020e6345",
    "outputId": "28a3ed3a-b9db-4140-bfde-3df694ce4710",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c927ac5d",
   "metadata": {
    "id": "c927ac5d"
   },
   "source": [
    "# Definition des Modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dab88f",
   "metadata": {
    "id": "41dab88f",
    "outputId": "2b916e2d-7324-47c2-f31c-25fe80c4c282"
   },
   "outputs": [],
   "source": [
    "# Pour référence, on considère la première série de ventes:\n",
    "P1['ordered_volumes'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8254c09e",
   "metadata": {
    "id": "8254c09e"
   },
   "outputs": [],
   "source": [
    "# On tronquera cette série aux alentours de février 2022, les données ne semblent pas pertinentes après cette date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85fbbc3",
   "metadata": {
    "id": "a85fbbc3"
   },
   "outputs": [],
   "source": [
    "#requiert des variables au format pd.DataFrame\n",
    "def Estimation_par_la_moyenne(Y_train, Y_test):\n",
    "    l_test = len(Y_test)\n",
    "    Y_mean = Y_train.mean()\n",
    "    RMSE_ref = (sum((Y_test - Y_mean)**2)/l_test)**(1/2)\n",
    "    print(RMSE_ref)\n",
    "    g = Y_test.plot()\n",
    "    g.figure.autofmt_xdate()\n",
    "    g.axhline(Y_mean, color='r')\n",
    "    \n",
    "def calc_rmse(Y_pred, Y_test):\n",
    "    return sqrt(mean_squared_error(Y_pred, Y_test))\n",
    "\n",
    "\n",
    "def model_fit(train, cfg):\n",
    "    if cfg[0] == 'univ_lasso':\n",
    "        train_lag = lagDf(train, cfg[1]).dropna()\n",
    "        X, Y = train_lag.iloc[:,1:], train_lag.iloc[:,0]\n",
    "        mdl = ElasticNet(l1_ratio=1, max_iter= 1500, alpha = cfg[2]).fit(X.to_numpy(),Y.to_numpy())\n",
    "        return mdl\n",
    "    elif cfg[0] == 'exp_smooth':\n",
    "        return\n",
    "    elif cfg[0] == 'univ_XGB':\n",
    "        train_lag = lagDf(train, cfg[1]).dropna()\n",
    "        X, Y = train_lag.iloc[:,1:], train_lag.iloc[:,0]\n",
    "        return XGBRegressor(n_estimators = cgf[1]).fit(X, Y)\n",
    "    elif cfg[0] == 'lasso_exo':\n",
    "        X, Y = train.iloc[:,1:], train.iloc[:,0]\n",
    "        mdl = ElasticNet(l1_ratio=1, max_iter= 1500, alpha = cfg[1]).fit(X.to_numpy(),Y.to_numpy())\n",
    "        return mdl\n",
    "    else:\n",
    "        print('Config invalide')\n",
    "        return\n",
    "\n",
    "def model_predict_st(model, history, cfg):\n",
    "    if cfg[0] == 'univ_lasso':\n",
    "        return model.predict(np.array(history[len(history) - cfg[1]:]).reshape(1, -1))\n",
    "    elif cfg[0] == 'exp_smooth':\n",
    "        return ExponentialSmoothing(history).fit().predict()[0]\n",
    "    elif cgf[0] == 'univ_XGB':\n",
    "        return\n",
    "\n",
    "def model_predict_exo(model, test, cfg):\n",
    "    if cfg[0] == 'lasso_exo':\n",
    "        X_test = test.iloc[:,1:]\n",
    "        return model.predict(X_test.to_numpy())\n",
    "    \n",
    "def univ_walk_forward_validation(data, n_test, cfg):\n",
    "    predictions = []\n",
    "    train, test = train_test_split(data, n_test=n_test)\n",
    "    model = model_fit(train, cfg)\n",
    "    if cfg[0] in ['univ_lasso', 'exp_smooth']:\n",
    "        history = [x for x in train]\n",
    "        for i in range(len(test)):\n",
    "            Yp = model_predict_st(model, history, cfg)\n",
    "            predictions.append(Yp)\n",
    "            history.append(test[i])\n",
    "        error = calc_rmse(test, predictions)\n",
    "    elif cfg[0] in ['lasso_exo']:\n",
    "        predictions = model_predict_exo(model, test, cfg).ravel()\n",
    "        error = calc_rmse(test.iloc[:,0], predictions)\n",
    "    #print(' > %.3f' % error)\n",
    "    return error\n",
    "    \n",
    "def repeat_evaluate(data, config, n_test, n_repeats = 3):\n",
    "    key = str(config)\n",
    "    scores = [univ_walk_forward_validation(data, n_test, config) for i in range(n_repeats)]\n",
    "    return (key, np.mean(scores))\n",
    "\n",
    "def grid_search(data, cfg_list, n_test):\n",
    "    scores = [repeat_evaluate(data, cfg, n_test) for cfg in cfg_list]\n",
    "    scores.sort(key=lambda tup: tup[1])\n",
    "    return scores\n",
    "\n",
    "# N'est pas pertinent car il utilise de la validation croisée inadaptée aux séries temporelles.\n",
    "#necessite des variables au format np.array\n",
    "def lasso(X, Xt, Y):\n",
    "    model = ElasticNet(l1_ratio=1, max_iter= 3000)\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "    grid = dict()\n",
    "    grid['alpha'] = logspace(-2,-1,50)\n",
    "    search = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "    results = search.fit(X, Y)\n",
    "    Yp = results.best_estimator_.predict(Xt)\n",
    "    return (results, Yp)\n",
    "\n",
    "def wfv_exp(train, n_test):\n",
    "    predictions = []\n",
    "    model = model_fit(train, ['exp_smooth'])\n",
    "    history = [x for x in train]\n",
    "    for i in range(n_test):\n",
    "        y = model_predict_st(model, history, ['exp_smooth'])\n",
    "        predictions.append(y)\n",
    "        history.append(y)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80844d44",
   "metadata": {
    "id": "80844d44"
   },
   "outputs": [],
   "source": [
    "# On décompose en saisonnalité + tendance + influence externe:\n",
    "#     - la saisonnalité reste inchangée\n",
    "#     - la tendance est étudiée idépendament des influences externes, on utilise un lissage exponentiel\n",
    "#     - les résidus sont prédits à l'aide des influences externes uniquement\n",
    "# On recombine le tout pour calculer l'erreur de ce modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6207e2e3",
   "metadata": {
    "id": "6207e2e3"
   },
   "outputs": [],
   "source": [
    "def lagNom(S, n):\n",
    "    nom = S.name\n",
    "    l = []\n",
    "    for i in range(n):\n",
    "        t = S.shift(7*(i+1), 'D') \n",
    "        t.name = nom + str(-(i+1))\n",
    "        l.append(t)\n",
    "    return pd.concat(l, axis = 1)\n",
    "\n",
    "def preprocess_et_decoupe(indice, n_test):\n",
    "    Data = l_df_without_lags[indice].set_index(\"time_index\").drop([\"forecasted_volumes\", \"mat_net_weight_value_kg\"], axis=1)[:\"20220201\"]\n",
    "    \n",
    "    temp_lag = lagNom(Data['apparenttemperaturemax'], 3)\n",
    "    precip_lag = lagNom(Data['precipintensity'], 1)\n",
    "    promo_lag = lagNom(Data['promo_uplift_coefficient'], 4)\n",
    "    promo_mean_lag = lagNom(Data['promo_mean_horizon_14'],1)\n",
    "    \n",
    "    Data = pd.concat([Data, temp_lag, precip_lag, promo_lag, promo_mean_lag], axis = 1).dropna()\n",
    "    Y = Data.pop('ordered_volumes')\n",
    "    Data.insert(0, 'ordered_volumes', Y)\n",
    "    \n",
    "    train, test = train_test_split(Data, n_test = n_test)\n",
    "    \n",
    "    decomp=seasonal_decompose(train['ordered_volumes'], model='Additive', period=54, two_sided = False, extrapolate_trend = 15)\n",
    "    decomp.plot()\n",
    "    Sais = decomp.seasonal\n",
    "    Trend = decomp.trend\n",
    "    Res = decomp.resid\n",
    "    \n",
    "    return train, test, Sais, Trend, Res\n",
    "\n",
    "def V1(train, test, Sais, Trend, Res):\n",
    "    train_X = train.iloc[:,1:]\n",
    "    Y_train = train.iloc[:,0]\n",
    "    #Prediction des residus -- lasso sur les variables exogènes\n",
    "    Res_results, Res_pred = lasso(train_X.values, test.iloc[:,1:].values, Res)\n",
    "    #Prediction de la tendance -- exponential smoothing sur la tendance\n",
    "    Trend_pred = wfv_exp(Trend, len(test))\n",
    "    #Prédiction à l'aide de la saisonnalité\n",
    "    S_pred = Sais.shift(54*7,'D').loc[test.index].to_numpy()\n",
    "    \n",
    "    #Somme des prédictions:\n",
    "    Y_pred = [S_pred[i] + Trend_pred[i] + Res_pred[i] for i in range(len(test))]\n",
    "    \n",
    "    return Trend_pred, Res_pred, S_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e325e0",
   "metadata": {
    "id": "03e325e0",
    "outputId": "57a2b5f6-9e2c-492d-edbc-8331a6fb705c"
   },
   "outputs": [],
   "source": [
    "preprocess_et_decoupe(1, 54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a96c250",
   "metadata": {
    "id": "4a96c250",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train, test, Sais, Trend, Res = preprocess_et_decoupe(1,54)\n",
    "rend_pred, Res_pred, S_pred = V1(train, test, Sais, Trend, Res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab6c93e",
   "metadata": {
    "id": "9ab6c93e",
    "outputId": "1344c578-bad1-42ef-f144-e52aa66ab3d5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(rend_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f774354",
   "metadata": {
    "id": "9f774354",
    "outputId": "d0a2b9d8-2c70-42cd-feda-047ae1552d64"
   },
   "outputs": [],
   "source": [
    "plt.plot(S_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336218ae",
   "metadata": {
    "id": "336218ae",
    "outputId": "c9eacf50-434b-4577-943c-0282559cd532"
   },
   "outputs": [],
   "source": [
    "plt.plot(Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d6786a",
   "metadata": {
    "id": "71d6786a"
   },
   "source": [
    "### Tests xgboost sur les variables exogènes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea00a8b",
   "metadata": {
    "id": "4ea00a8b",
    "outputId": "898e8bd9-6388-4a0d-82ea-a42d23d11cad"
   },
   "outputs": [],
   "source": [
    "(X_train, X_test, Y_train, Y_test) = preprocess(P1[:\"20220201\"], n_test = 27)\n",
    "xgb = XGBRegressor(n_estimators=1500)\n",
    "xgb.fit(X_train, Y_train)\n",
    "Yp = xgb.predict(X_test)\n",
    "data = {'Y':Y_test.tolist(), 'Y estimé': Yp.tolist()}\n",
    "pd.DataFrame(data, index = Y_test.index).plot()\n",
    "calc_rmse(Yp, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a627169e",
   "metadata": {
    "id": "a627169e"
   },
   "source": [
    "### Estimation par la moyenne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3ec9da",
   "metadata": {
    "id": "fd3ec9da",
    "outputId": "5d89b231-b75e-4fa3-9d4c-b93d0304c43d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(X_train, X_test, Y_train, Y_test) = preprocess(P1[:\"20220201\"], n_test = 27)\n",
    "Estimation_par_la_moyenne(Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf890cb",
   "metadata": {
    "id": "eaf890cb"
   },
   "source": [
    "### Lasso sans données exogènes\n",
    "Prédictions sur les n_out derniers mois, avec n_in an de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928acc57",
   "metadata": {
    "id": "928acc57",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lasso_ts(data, n_out, n_in):\n",
    "    g = logspace(3, 3.5, 50)\n",
    "    cfg_list = [['univ_lasso', n_in, g[i]] for i in range(len(g))]\n",
    "    scores = grid_search(data, cfg_list, n_out)\n",
    "    #print('done')\n",
    "    for cfg, error in scores[:10]:\n",
    "        print(cfg, error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9cfcd8",
   "metadata": {
    "id": "6a9cfcd8"
   },
   "outputs": [],
   "source": [
    "#sur 6 mois avec un an de données d'entrée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6503627",
   "metadata": {
    "id": "b6503627",
    "outputId": "f9e654ff-f21a-4e06-a497-c75afc82206f"
   },
   "outputs": [],
   "source": [
    "lasso_ts(P1[:\"20220201\"]['ordered_volumes'], 27, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d01da2",
   "metadata": {
    "id": "e9d01da2"
   },
   "outputs": [],
   "source": [
    "#sur 6 mois avec deux an de données d'entrée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7738031e",
   "metadata": {
    "id": "7738031e",
    "outputId": "4c0c8754-ad26-4291-f253-a35c23fedf36"
   },
   "outputs": [],
   "source": [
    "lasso_ts(P1[:\"20220201\"]['ordered_volumes'], 27, 108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70787f5a",
   "metadata": {
    "id": "70787f5a",
    "outputId": "088833cc-e896-48e0-fb9f-c3c64a27c464"
   },
   "outputs": [],
   "source": [
    "lasso_ts(P1[:\"20220201\"]['ordered_volumes'], 1, 54)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd4e59b",
   "metadata": {
    "id": "1fd4e59b"
   },
   "source": [
    "### LASSO uniquement sur les variables exogènes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b09884b",
   "metadata": {
    "id": "4b09884b"
   },
   "outputs": [],
   "source": [
    "def lasso_exo(data, n_out):\n",
    "    g = logspace(0.5, 1.5, 50)\n",
    "    cfg_list = [['lasso_exo', g[i]] for i in range(len(g))]\n",
    "    scores = grid_search(data, cfg_list, n_out)\n",
    "    #print('done')\n",
    "    for cfg, error in scores[:10]:\n",
    "        print(cfg, error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bb12c3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "44bb12c3",
    "outputId": "86d55ad0-c8da-409f-d2c9-55c6a2d1fc31"
   },
   "outputs": [],
   "source": [
    "Data = P1.copy()\n",
    "Y = Data.pop('ordered_volumes')\n",
    "Data.insert(0, 'ordered_volumes', Y)\n",
    "lasso_exo(Data, 54)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d645d2",
   "metadata": {
    "id": "10d645d2"
   },
   "source": [
    "## LASSO \"Grossiers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34c6d89",
   "metadata": {
    "id": "a34c6d89"
   },
   "source": [
    "### LASSO uniquement sur les variables exogènes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d21b74",
   "metadata": {
    "id": "d7d21b74",
    "outputId": "4124417f-4f71-449e-9c47-ac763ef2198c"
   },
   "outputs": [],
   "source": [
    "(X_train, X_test, Y_train, Y_test) = preprocess(P1[:\"20220201\"], n_test = 54)\n",
    "results, Yp = lasso(X_train.to_numpy(), X_test.to_numpy(), Y_train.to_numpy())\n",
    "calc_rmse(Y_test.to_numpy(), Yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3db6e0",
   "metadata": {
    "id": "ab3db6e0",
    "outputId": "247fb258-74c9-4b70-fdf1-261aa74a8f80",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = {'Y':Y_test.tolist(), 'Y estimé': Yp.tolist()}\n",
    "pd.DataFrame(data, index = Y_test.index).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfdc1df",
   "metadata": {
    "id": "fdfdc1df",
    "outputId": "27b9d0be-ee79-4b65-a214-c61ca96fed57"
   },
   "outputs": [],
   "source": [
    "results.best_estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdedcff",
   "metadata": {
    "id": "abdedcff"
   },
   "outputs": [],
   "source": [
    "#Prédicteurs les plus importants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfab2b4",
   "metadata": {
    "id": "2cfab2b4",
    "outputId": "072dc7c2-5db2-411b-f46d-3be1e1e88a3d"
   },
   "outputs": [],
   "source": [
    "Coefs = abs(pd.DataFrame(results.best_estimator_.coef_))\n",
    "Index = []\n",
    "for i in range(8):\n",
    "    Index.append(int(Coefs.idxmax()))\n",
    "    Coefs[0][Index[i]] = 0\n",
    "X_train.columns[Index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4c13fa",
   "metadata": {
    "id": "4b4c13fa"
   },
   "source": [
    "### LASSO prenant en compte un an de données de ventes\n",
    "Les résultats sont obtenus \"en trichant\" sur les valeurs de ventes de la période test.\n",
    "En effet, pour faire des prédiction à horizon t>1, on utilise les ventes réelles à t-k et non les valeurs déjà prédites.\n",
    "Les résultats sont assez mauvais comme ça pour ne pas corriger ce problème."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3abe430",
   "metadata": {
    "id": "c3abe430",
    "outputId": "cb6b0524-8201-4bf1-f15f-d788b584e42c"
   },
   "outputs": [],
   "source": [
    "(X_train, X_test, Y_train, Y_test) = preprocess(P1[:\"20220201\"], method = 'lags')\n",
    "results, Yp = lasso(X_train.to_numpy(), X_test.to_numpy(), Y_train.to_numpy())\n",
    "calc_rmse(Y_test.to_numpy(), Yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37beab63",
   "metadata": {
    "id": "37beab63",
    "outputId": "d7565d33-c921-4145-ddba-b4d489b08c42"
   },
   "outputs": [],
   "source": [
    "data = {'Y':Y_test.tolist(), 'Y estimé': Yp.tolist()}\n",
    "pd.DataFrame(data, index = Y_test.index).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eed36a2",
   "metadata": {
    "id": "6eed36a2",
    "outputId": "628013c5-41a4-4227-b064-8fe760e52f72",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.best_estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f80b7d8",
   "metadata": {
    "id": "2f80b7d8",
    "outputId": "cf4c7487-83da-4a62-c8ef-00aadbb66dd2"
   },
   "outputs": [],
   "source": [
    "Coefs = abs(pd.DataFrame(results.best_estimator_.coef_))\n",
    "Index = []\n",
    "for i in range(20):\n",
    "    Index.append(int(Coefs.idxmax()))\n",
    "    Coefs[0][Index[i]] = 0\n",
    "X_train.columns[Index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4cc7c0",
   "metadata": {
    "id": "1b4cc7c0"
   },
   "source": [
    "### LASSO sur une série mal stationnarisée à la main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5607d8aa",
   "metadata": {
    "id": "5607d8aa"
   },
   "outputs": [],
   "source": [
    "(X_train, X_test, Y_train, Y_test) = preprocess(P1, method = 'stationnarisation')\n",
    "results, Yp = lasso(X_train.to_numpy(), X_test.to_numpy(), Y_train.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e84633d",
   "metadata": {
    "id": "8e84633d",
    "outputId": "1e0f2663-2a5f-4261-d3f5-5f6046b12fe8"
   },
   "outputs": [],
   "source": [
    "calc_rmse(Y_test.to_numpy(), Yp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd3939f",
   "metadata": {
    "id": "abd3939f"
   },
   "source": [
    "### Modèle AR(1) + LASSO\n",
    "On considère un modèle AR(1) pour $Z(t) = \\frac{1}{3}\\sum_{i=0}^{2}Y(t-i)$:\n",
    "\n",
    "$Z(t) = c + \\phi Z(t-1) + \\epsilon_{t}$\n",
    "\n",
    "Il s'ensuit:\n",
    "\n",
    "$Y(t) = 3c + (\\phi - 1)(Y(t-1) + Y(t-2)) + \\phi Y(t-3) + 3\\epsilon_{t}$\n",
    "\n",
    "On effectuera une régression supplémentaire sur $\\epsilon_{t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ce6c70",
   "metadata": {
    "id": "a1ce6c70"
   },
   "outputs": [],
   "source": [
    "(X_train, X_test, Y_train, Y_test) = preprocess(P1[:\"20220201\"])\n",
    "mY = movAvg(Y_train, 3)\n",
    "mY_num = mY.to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c221ab8b",
   "metadata": {
    "id": "c221ab8b",
    "outputId": "f1f6f0b6-ee15-4790-df43-1fafa7ce5267",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = sum(mY_num[i+4] - mY_num[i+3] for i in range(len(mY) - 5))/(len(mY) - 5)\n",
    "phi = acf(mY)[1]\n",
    "(c, phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848dcf99",
   "metadata": {
    "id": "848dcf99"
   },
   "outputs": [],
   "source": [
    "Y_num = Y_train.to_numpy().ravel()\n",
    "Y_h = []\n",
    "for i in range(3,len(Y_num)):\n",
    "    Y_h.append(float(3*c + (phi-1)*(Y_num[i-1]+Y_num[i-2]) + phi*Y_num[i-3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e113e145",
   "metadata": {
    "id": "e113e145",
    "outputId": "5c205f2c-84d2-46d4-8558-7d5f454bd138",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = {'Y':Y_num.tolist()[:-3], 'Y estimé': Y_h}\n",
    "pd.DataFrame(data, index = T[3:len(Y_train)]).plot(figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5b79d1",
   "metadata": {
    "id": "fc5b79d1",
    "outputId": "27808248-4aec-4a26-bf33-4f7461664b45",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ratio = [Y_num.tolist()[i]/Y_h[i] for i in range(len(Y_h))]\n",
    "plt.plot(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd96ac7",
   "metadata": {
    "id": "fbd96ac7",
    "outputId": "f5fb71c0-8d3e-45c9-aba2-f98b211e5248"
   },
   "outputs": [],
   "source": [
    "(np.var(ratio), np.mean(ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b419a623",
   "metadata": {
    "id": "b419a623",
    "outputId": "1e42b621-05ff-44bc-a825-5a31cd69e71c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s, j = 0, 0\n",
    "for i in range(len(ratio)):\n",
    "    if(abs(ratio[i]-1.65) < 1):\n",
    "        s = s+ratio[i]\n",
    "        j = j+1\n",
    "s/j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c919900f",
   "metadata": {
    "id": "c919900f",
    "outputId": "81101a73-6a1e-41b2-f14b-52388b8fbc3f"
   },
   "outputs": [],
   "source": [
    "Y_h_corr = [1.65*Y_h[i] for i in range(len(Y_h))]\n",
    "EPS = [Y_num.tolist()[i] - Y_h_corr[i] for i in range(len(Y_h))]\n",
    "plt.plot(EPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84608b36",
   "metadata": {
    "id": "84608b36"
   },
   "outputs": [],
   "source": [
    "#servons nous de ce modèle pour établir une prédiction\n",
    "(X_train, X_test, Y_train, Y_test) = preprocess(P1[:\"20220201\"])\n",
    "Y_t = Y_train.to_numpy().ravel()\n",
    "Yp = [float(3*c + (phi-1)*(Y_t[-1]+Y_t[-2]) + phi*Y_t[-3])]\n",
    "Yp.append(float(3*c + (phi-1)*(Yp[0]+Y_t[-1]) + phi*Y_t[-2]))\n",
    "Yp.append(float(3*c + (phi-1)*(Yp[1]+Yp[0]) + phi*Y_t[-1]))\n",
    "for i in range(3, len(Y_test)-38):\n",
    "    Yp.append(1.65*float(3*c + (phi-1)*(Yp[i-1]+Yp[i-2]) + phi*Yp[i-3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ddaed8",
   "metadata": {
    "id": "b6ddaed8",
    "outputId": "5d0e15ce-fee0-4214-f93c-199368f42929"
   },
   "outputs": [],
   "source": [
    "len(Yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1724e0",
   "metadata": {
    "id": "5c1724e0",
    "outputId": "0c2f5221-d367-47bc-c133-48662fc46497",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = {'Y':Y_test.tolist()[:-38], 'Y estimé': Yp}\n",
    "pd.DataFrame(data, index = Y_test.index[:-38]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecddc339",
   "metadata": {
    "id": "ecddc339"
   },
   "outputs": [],
   "source": [
    "#A faire: calculer le coefficient correcteur\n",
    "def AR_LASSO(Data, h = 10):\n",
    "    #Prétraitement\n",
    "    (X_train, X_test, Y_train, Y_test) = preprocess(Data[:\"20220201\"])\n",
    "    if(h>len(Y_test)):\n",
    "        h = len(Y_test)\n",
    "    if(h<3):\n",
    "        h=3\n",
    "    X_t = X_test.to_numpy()\n",
    "    Y_t = Y_train.to_numpy().ravel()\n",
    "    #Calcul des paramètres\n",
    "    mY = movAvg(Y_train, 3)\n",
    "    mY_num = mY.to_numpy().ravel()\n",
    "    c = sum(mY_num[i+4] - mY_num[i+3] for i in range(len(mY) - 5))/(len(mY) - 5)\n",
    "    phi = acf(mY)[1]\n",
    "    #Prétraitement pour le LASSO\n",
    "    Y_h = []\n",
    "    for i in range(3,len(Y_t)):\n",
    "        Y_h.append(float(3*c + (phi-1)*(Y_t[i-1]+Y_t[i-2]) + phi*Y_t[i-3]))\n",
    "    Eps = [Y_t[i] - 1.65*Y_h[i] for i in range(len(Y_h))]\n",
    "    #Optimisation du LASSO\n",
    "    results, Yp = lasso(X_train.to_numpy()[3:], X_t, Eps)\n",
    "    #predictions\n",
    "    coefs = results.best_estimator_.coef_\n",
    "    \n",
    "    #Initialisation des prédictions\n",
    "    Yp = [float(1.65*(3*c + (phi-1)*(Y_t[-1]+Y_t[-2]) + phi*Y_t[-3]) + np.dot(X_t[0], coefs))]\n",
    "    Yp.append(float(1.65*(3*c + (phi-1)*(Yp[0]+Y_t[-1]) + phi*Y_t[-2]))+ np.dot(X_t[1], coefs))\n",
    "    Yp.append(1.65*(float(3*c + (phi-1)*(Yp[1]+Yp[0]) + phi*Y_t[-1]))+ np.dot(X_t[2], coefs))\n",
    "    for i in range(3, h):\n",
    "        Yp.append(1.65*float(3*c + (phi-1)*(Yp[i-1]+Yp[i-2]) + phi*Yp[i-3])+ np.dot(X_t[i], coefs))\n",
    "        \n",
    "    data = {'Y':Y_test.tolist()[:h], 'Y estimé': Yp}\n",
    "    pd.DataFrame(data, index = Y_test.index[:h]).plot()\n",
    "    print(calc_rmse(Y_test.tolist()[:h], Yp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e49037",
   "metadata": {
    "id": "58e49037",
    "outputId": "d48bf9ba-120c-4bd6-80f3-29079cd1e6e1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AR_LASSO(P1, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70e8023",
   "metadata": {
    "id": "f70e8023"
   },
   "source": [
    "### Saisonnalité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9d88cb",
   "metadata": {
    "id": "ec9d88cb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69122801",
   "metadata": {
    "id": "69122801",
    "outputId": "2f04b84a-09c6-44e3-a5c7-f4bdff6638f0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = P1[:\"20220201\"]\n",
    "Y = P1[\"ordered_volumes\"][:\"20220201\"]\n",
    "result=seasonal_decompose(Y, model='Additive', period=54, two_sided = False, extrapolate_trend = 15)\n",
    "result.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510cf2ff",
   "metadata": {
    "id": "510cf2ff",
    "outputId": "221677f5-7fea-4bd8-c8b3-32fbd29dc2a2"
   },
   "outputs": [],
   "source": [
    "result.trend.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cc9039",
   "metadata": {
    "id": "44cc9039",
    "outputId": "ed2b1ec1-44c0-445c-bc36-245c3a961545"
   },
   "outputs": [],
   "source": [
    "Trend = result.trend.dropna()\n",
    "Tr_train, Tr_test = train_test_split(Trend, None)\n",
    "(X_train, X_test, A, B) = preprocess(X.loc[Trend.index])\n",
    "results, Tr_p = lasso(X_train.to_numpy(), X_test.to_numpy(), Tr_train.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371e1f92",
   "metadata": {
    "id": "371e1f92",
    "outputId": "322e9c35-4c75-4ec7-9014-90685c6f7d90"
   },
   "outputs": [],
   "source": [
    "calc_rmse(Tr_p, Tr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9647f979",
   "metadata": {
    "id": "9647f979"
   },
   "outputs": [],
   "source": [
    "Resid = result.resid.dropna()\n",
    "R_train, R_test = train_test_split(Resid)\n",
    "resultsR, R_p = lasso(X_train.to_numpy(), X_test.to_numpy(), R_train.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a332a4dc",
   "metadata": {
    "id": "a332a4dc",
    "outputId": "f9dad61e-95f4-459e-9ab1-165ab8465ab9"
   },
   "outputs": [],
   "source": [
    "calc_rmse(R_p, R_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f7ed7c",
   "metadata": {
    "id": "91f7ed7c",
    "outputId": "69e71087-1c06-44be-e018-3c0547b14d25"
   },
   "outputs": [],
   "source": [
    "resultsR, P_p = lasso(X_train.to_numpy(), X_test.to_numpy(), [R_train.to_numpy()[i]+Tr_train.to_numpy()[i] for i in range(len(R_train.index))])\n",
    "calc_rmse([R_test.to_numpy()[i]+Tr_test.to_numpy()[i] for i in range(len(R_test.index))], P_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4967d2d0",
   "metadata": {
    "id": "4967d2d0",
    "outputId": "c21957c4-606d-49d1-877e-30be1d935148"
   },
   "outputs": [],
   "source": [
    "Y_p = [P_p[i] + Sais[i] for i in range(len(R_test.index))]\n",
    "Y_test = P1[\"ordered_volumes\"].loc[R_test.index]\n",
    "data = {'Y':Y_test.tolist(), 'Y estimé': Y_p}\n",
    "pd.DataFrame(data, index = Y_test.index).plot()\n",
    "print(calc_rmse(Y_test, Y_p))\n",
    "print(len(R_test.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f427f4e5",
   "metadata": {
    "id": "f427f4e5",
    "outputId": "2845ed90-f573-46a4-f8d7-012ddb900948"
   },
   "outputs": [],
   "source": [
    "Sais = result.seasonal.shift(54*7,'D').loc[R_test.index].to_numpy()\n",
    "\n",
    "Y_p = [R_p[i] + Tr_p[i] + Sais[i] for i in range(len(R_test.index))]\n",
    "Y_test = P1[\"ordered_volumes\"].loc[R_test.index]\n",
    "data = {'Y':Y_test.tolist(), 'Y estimé': Y_p}\n",
    "pd.DataFrame(data, index = Y_test.index).plot()\n",
    "print(calc_rmse(Y_test, Y_p))\n",
    "print(len(R_test.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ddec5e",
   "metadata": {
    "id": "45ddec5e"
   },
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b90c20",
   "metadata": {
    "id": "f7b90c20",
    "outputId": "7e9c7ca3-fef0-453c-9a4f-dcf655e405af"
   },
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[0]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols = list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(7*i, 'D'))\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "            cols.append(df.shift(-7*i, 'D'))\n",
    "        # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg.values\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def train_test_split(data, n_test):\n",
    "    return data[:-n_test, :], data[-n_test:, :]\n",
    "\n",
    "# fit an xgboost model and make a one step prediction\n",
    "def xgboost_forecast(train, testX):\n",
    "    # transform list into array\n",
    "    train = np.asarray(train)\n",
    "    # split into input and output columns\n",
    "    trainX, trainy = train[:, :-1], train[:, -1]\n",
    "    # fit model\n",
    "    model = XGBRegressor(objective='reg:squarederror', n_estimators=1000)\n",
    "    model.fit(trainX, trainy)\n",
    "    # make a one-step prediction\n",
    "    yhat = model.predict(np.asarray([testX]))\n",
    "    return yhat[0]\n",
    "\n",
    "# walk-forward validation for univariate data\n",
    "def walk_forward_validation(data, n_test):\n",
    "    predictions = list()\n",
    "    # split dataset\n",
    "    train, test = train_test_split(data, n_test)\n",
    "    # seed history with training dataset\n",
    "    history = [x for x in train]\n",
    "    # step over each time-step in the test set\n",
    "    for i in range(len(test)):\n",
    "        # split test row into input and output columns\n",
    "        testX, testy = test[i, :-1], test[i, -1]\n",
    "        # fit model on history and make a prediction\n",
    "        yhat = xgboost_forecast(history, testX)\n",
    "        # store forecast in list of predictions\n",
    "        predictions.append(yhat)\n",
    "        # add actual observation to history for the next loop\n",
    "        history.append(test[i])\n",
    "        # summarize progress\n",
    "        #print('>expected=%.1f, predicted=%.1f' % (testy, yhat))\n",
    "    # estimate prediction error\n",
    "    error = mean_squared_error(test[:, -1], predictions)\n",
    "    return error, test[:, -1], predictions\n",
    "\n",
    "\n",
    "data = series_to_supervised(P1[:\"20220201\"][\"ordered_volumes\"], n_in=54)\n",
    "\n",
    "mse, y, yhat = walk_forward_validation(data, 54)\n",
    "calc_rmse = sqrt(mse)\n",
    "print('RMSE: %.3f' % calc_rmse)\n",
    "\n",
    "plt.plot(y, label='Expected')\n",
    "plt.plot(yhat, label='Predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94743367",
   "metadata": {
    "id": "94743367",
    "outputId": "125488ff-1828-46a4-ed4c-37aa2f3267e2"
   },
   "outputs": [],
   "source": [
    "In = [27, 108]\n",
    "Out = [1, 4, 13, 27, 54]\n",
    "for i in In:\n",
    "    for j in Out:\n",
    "        data = series_to_supervised(P1[:\"20220201\"][\"ordered_volumes\"], n_in=i)\n",
    "        mse, y, yhat = walk_forward_validation(data, j)\n",
    "        calc_rmse = sqrt(mse)\n",
    "        print(str(i) + ',' + str(j) +' RMSE: %.3f' % calc_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9900132b",
   "metadata": {
    "id": "9900132b"
   },
   "source": [
    "# Tests en vrac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0362015",
   "metadata": {
    "id": "b0362015",
    "outputId": "6f18577d-62b3-4b7b-80bd-c95c5feaf4cf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = sns.relplot(x=\"time_index\", y=\"ordered_volumes\", data=P1_train, kind=\"line\")\n",
    "g = sns.relplot(x=\"time_index\", y=\"rolling_mean_ordered_volumes_minus_50_to_54\", data=P1_train, kind=\"line\", color ='r')\n",
    "g.figure.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fecc33",
   "metadata": {
    "id": "c3fecc33",
    "outputId": "bd183c15-392c-48e7-ce16-1e0048837d92",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = X_train.to_numpy()\n",
    "Y = Y_train.to_numpy()\n",
    "Xt = X_test.to_numpy()\n",
    "Yt = Y_test.to_numpy()\n",
    "\n",
    "model = ElasticNet(l1_ratio=1, max_iter= 3000)\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "grid = dict()\n",
    "grid['alpha'] = logspace(-2,-1,50)\n",
    "search = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "results = search.fit(X, Y)\n",
    "Yp = results.best_estimator_.predict(Xt)\n",
    "calc_rmse(Yt, Yp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eb1d40",
   "metadata": {
    "id": "96eb1d40",
    "outputId": "6ac926d0-214a-4a00-a3bc-a511fdd409d4"
   },
   "outputs": [],
   "source": [
    "Coefs = abs(pd.DataFrame(results.best_estimator_.coef_))\n",
    "Index = []\n",
    "for i in range(10):\n",
    "    Index.append(int(Coefs.idxmax()))\n",
    "    Coefs[0][Index[i]] = 0\n",
    "X_train.columns[Index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e571b64",
   "metadata": {
    "id": "8e571b64",
    "outputId": "7404bb89-efce-4430-b2c0-23b845a5f3f0"
   },
   "outputs": [],
   "source": [
    "results.best_estimator_.coef_[Index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74c9f54",
   "metadata": {
    "id": "e74c9f54",
    "outputId": "9a237370-2843-4b81-9fe0-168061196f7b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    (P1_train, P1_test, X_train, Y_train, X_test, l_train, l_test) = Partitionnement(Produits[i])\n",
    "    print(\"RMSE \" + str(i+1) + \":\" + str(LASSO_all_var(X_train, Y_train, X_test, Y_test, p=500)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e8cfb3",
   "metadata": {
    "id": "45e8cfb3"
   },
   "source": [
    "## Modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ad9c0",
   "metadata": {
    "id": "715ad9c0"
   },
   "source": [
    "On considère un modèle AR(1) pour $Z(t) = \\frac{1}{3}\\sum_{i=0}^{2}Y(t-i)$:\n",
    "\n",
    "$Z(t) = c + \\phi Z(t-1) + \\epsilon_{t}$\n",
    "\n",
    "Il s'ensuit:\n",
    "\n",
    "$Y(t) = 3c + (\\phi - 1)(Y(t-1) + Y(t-2)) + \\phi Y(t-3) + 3\\epsilon_{t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d542100",
   "metadata": {
    "id": "3d542100"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
