{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d10dc1",
   "metadata": {
    "id": "e8d10dc1",
    "outputId": "953c5b0b-9470-4b28-95c9-179dd762c984"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from openpyxl import Workbook\n",
    "import string\n",
    "import shap\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from scipy.stats import bernoulli\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d78105",
   "metadata": {
    "id": "11d78105"
   },
   "source": [
    "## Importation des données et prétraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f755dd43",
   "metadata": {
    "id": "f755dd43",
    "outputId": "58dd3a51-b93a-4eb8-852a-6cea72de4baa"
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet()#ECRIRE L'EMPLACEMENT DES DONNEES ICI\n",
    "df.fillna(method ='pad',inplace = True)\n",
    "df.drop(['location_index','customer_index'], inplace=True, axis=1)\n",
    "min_max_scalers = [] #List of scalers for rescalling purposes\n",
    "l_df=[]#list of the whole dataframe grouped per product\n",
    "l_df_without_lags=[]#we remove lags\n",
    "l_df_without_lags_scaled=[]#we remove lags and rescale\n",
    "number=[]\n",
    "products=pd.unique(df['product_index'])\n",
    "#Décompositions en liste de DataFrames par produits\n",
    "for index in range(len(products)):\n",
    "    df_temp=df[df['product_index']==products[index]].sort_values(by=\"time_index\")\n",
    "    l_df.append(df_temp)\n",
    "    number.append(df_temp.shape)\n",
    "    #Selection des variables étudiées\n",
    "    df_temp_without_lags=pd.DataFrame({\n",
    "        'time_index' : df_temp['time_index'],\n",
    "        'apparenttemperaturemax':df_temp['apparenttemperaturemax_minus_1'],\n",
    "        'cos_iso_month':df_temp['cos_iso_month'],\n",
    "        'cos_iso_week':df_temp['cos_iso_week'],\n",
    "        'cos_iso_week_of_month':df_temp['cos_iso_week_of_month'],\n",
    "        'days_before_next_holiday':df_temp['days_before_next_holiday'],\n",
    "        'forecasted_volumes' :df_temp['forecasted_volumes'],\n",
    "        'holiday_day_of_week':df_temp['holiday_day_of_week'],\n",
    "        'holidays_count_in_week':df_temp['holidays_count_in_week'],\n",
    "        'iso_month':df_temp['iso_month'],\n",
    "        'iso_week':df_temp['iso_week'],\n",
    "        'promo_mean_horizon_14':df_temp['promo_mean_horizon_14'],\n",
    "        'iso_week_of_month':df_temp['iso_week_of_month'],\n",
    "        'mat_net_weight_value_kg':df_temp['mat_net_weight_value_kg'],\n",
    "        'ordered_volumes':df_temp['ordered_volumes'],\n",
    "        'precipintensity':df_temp['precipintensity_minus_1'],\n",
    "        'promo_uplift_coefficient':df_temp['promo_uplift_coefficient']\n",
    "    })\n",
    "    l_df_without_lags.append(df_temp_without_lags)\n",
    "    df_temp_without_lags_rescaled=df_temp_without_lags.copy(deep=True)\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    min_max_scalers.append(min_max_scaler)\n",
    "    df_temp_without_lags_rescaled[['forecasted_volumes','ordered_volumes','apparenttemperaturemax','precipintensity']]=min_max_scalers[-1].fit_transform(df_temp_without_lags[['forecasted_volumes','ordered_volumes','apparenttemperaturemax','precipintensity']])\n",
    "    l_df_without_lags_scaled.append(df_temp_without_lags_rescaled)\n",
    "\n",
    "#Index permettant de lier numéro d'un produit et ordre dans la liste\n",
    "Product_index = {}\n",
    "for i in range(len(l_df)):\n",
    "    Product_index[str(l_df[i]['product_index'].values[0])] = i\n",
    "#Selection d'un produit dans la liste\n",
    "def indice_to_df(indice, scaled = True):\n",
    "    if scaled:\n",
    "        return l_df_without_lags_scaled[indice].set_index(\"time_index\").drop([\"forecasted_volumes\", \"mat_net_weight_value_kg\"], axis=1)[:\"20220201\"]\n",
    "    return l_df_without_lags[indice].set_index(\"time_index\").drop([\"forecasted_volumes\", \"mat_net_weight_value_kg\"], axis=1)[:\"20220201\"]\n",
    "# Convertit une liste de numréos de produits en une liste d'indices\n",
    "def list_to_list(L):\n",
    "    T = []\n",
    "    for i in L:\n",
    "        T.append(Product_index[str(i)])\n",
    "    return T\n",
    "#Division en train-test    \n",
    "def train_test_split(Data, ratio_train_test = 0.8, n_test = None):\n",
    "    if n_test == None:\n",
    "        n_test = int(len(Data)*(1-ratio_train_test))\n",
    "    if(n_test < len(Data)):\n",
    "        return Data[:len(Data) - n_test], Data[len(Data) - n_test:]\n",
    "    print('nombre de données test trop grand')\n",
    "    return None, Data\n",
    "#Permet d'introduire un décalage dans les données avec un nom approprié\n",
    "def lagNom(S, n):\n",
    "    nom = S.name\n",
    "    l = []\n",
    "    for i in range(n):\n",
    "        t = S.shift(7*(i+1), 'D') \n",
    "        t.name = nom + str(-(i+1))\n",
    "        l.append(t)\n",
    "    return pd.concat(l, axis = 1)\n",
    "#Fonction transformant une variable catégorielle en variables binaires\n",
    "def toCat(S):\n",
    "    nom = S.name\n",
    "    S_u = pd.unique(S)\n",
    "    l = []\n",
    "    for value in S_u:\n",
    "        t = (S == value).astype(int)\n",
    "        t.name = nom + \"=\" + str(value)\n",
    "        l.append(t)\n",
    "    return pd.concat(l, axis=1)\n",
    "#Fonction de préprocessing pour faire des prévisions de données à horizon 1\n",
    "# Permet de transformer les données pourfaire de l'apprentissage supervisé\n",
    "def preprocess_(Data, Out = 17, In = 0, Exo = True, Cat = True):\n",
    "    # Introductions de lags sur la variable à prédire\n",
    "    Y = Data.pop('ordered_volumes')\n",
    "    if In > 0:\n",
    "        Y_lag = lagNom(Y, In)\n",
    "    # Transformation des variables catégorielles en variables binaires\n",
    "    if Cat:\n",
    "        C = [Data]\n",
    "        C.append(toCat(Data.pop('iso_month')))\n",
    "        C.append(toCat(Data.pop('iso_week')))\n",
    "        C.append(toCat(Data.pop('iso_week_of_month')))\n",
    "        Data = pd.concat(C, axis = 1)\n",
    "    # Introductions de lags sur les variables exogènes\n",
    "    if Exo:\n",
    "        temp_lag = lagNom(Data['apparenttemperaturemax'], 3)\n",
    "        precip_lag = lagNom(Data['precipintensity'], 1)\n",
    "        promo_lag = lagNom(Data['promo_uplift_coefficient'], 4)\n",
    "        promo_mean_lag = lagNom(Data['promo_mean_horizon_14'],1)\n",
    "        Data = pd.concat([Y, Data, temp_lag, precip_lag, promo_lag, promo_mean_lag, Y_lag], axis = 1).dropna()\n",
    "    else:\n",
    "        Data = pd.concat([Y, Y_lag], axis = 1).dropna()\n",
    "    \n",
    "    return train_test_split(Data, n_test = Out)\n",
    "\n",
    "def rmse(Y_pred, Y_test):\n",
    "    return np.sqrt(mean_squared_error(Y_pred, Y_test))\n",
    "\n",
    "# Fonction de prétraitement des données permettant un apprentissage sur l'ensemble des séries\n",
    "# Renvoie les données nécessaires pour un apprentissage supervisé\n",
    "# - La variable i_test indique la série utilisée pour le test\n",
    "# - La variable Exo permet de choisir ou non d'inclure les variables exogènes\n",
    "# - La variable Cat permet de transformer les variables catégorielles en variables binaires\n",
    "# - La variable filtre permet de supprimer des variables inintéressantes\n",
    "def preprocess_list(Indices, i_test, Exo = True, Cat = False, Index = False, Filtre = None):\n",
    "    L_X, L_Y = [],[]\n",
    "    condition = True\n",
    "    for indice in Indices:\n",
    "        if len(l_df_without_lags_scaled[indice])>200:\n",
    "            train, test = preprocess_(indice_to_df(indice), Out = 14, In = 60, Exo = Exo, Cat = Cat)\n",
    "            X_temp = train.iloc[:,1:].reset_index().drop([\"time_index\"], axis=1)\n",
    "            if Index:\n",
    "                  X_temp[\"product_index\"] = l_df[indice]['product_index'].values[0]\n",
    "            L_X.append(X_temp)\n",
    "            L_Y.append(train.iloc[:,0].reset_index().drop([\"time_index\"], axis=1))\n",
    "            if indice == i_test:\n",
    "                condition = False\n",
    "                X_test = test.iloc[:,1:]\n",
    "                if Index:\n",
    "                    X_test[\"product_index\"] = l_df[indice]['product_index'].values[0]\n",
    "                Y_test = test.iloc[:,0]\n",
    "    X_train = pd.concat(L_X)\n",
    "    if Index:\n",
    "        C = [X_train]\n",
    "        C.append(toCat(X_train.pop(\"product_index\")))\n",
    "        X_train = pd.concat(C, axis = 1)\n",
    "    Y_train = pd.concat(L_Y)\n",
    "    if Filtre != None:\n",
    "        for c in Filtre:\n",
    "            X_train.drop([c], axis=1)\n",
    "            X_test.drop([c], axis=1)\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "def preprocess_test(i, Exo = True, Cat = False):\n",
    "    train, test = preprocess_(indice_to_df(i), Out = 14, In = 60, Exo = Exo, Cat = Cat)\n",
    "    return test.iloc[:,1:], test.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d23959",
   "metadata": {
    "id": "c7d23959"
   },
   "source": [
    "On notera que l'on travail avec des données normalisées, pour ne pas normaliser les données il faut modifier la condition $\\textit{scaled}$ de la fonction indice_to_df."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e97396",
   "metadata": {
    "id": "45e97396"
   },
   "source": [
    "## Modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02670610",
   "metadata": {
    "id": "02670610"
   },
   "outputs": [],
   "source": [
    "def get_a_model(methode):\n",
    "    if methode == \"lin\":\n",
    "        model = ElasticNet(max_iter= 5000)\n",
    "        cv = TimeSeriesSplit(n_splits=5)\n",
    "        grid = {\n",
    "            'alpha': np.logspace(-2,1,100),\n",
    "            'l1_ratio': np.linspace(0.1, 1, 10)\n",
    "        }\n",
    "        return GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "    if methode ==\"xgb\":\n",
    "        estimator = xgb.XGBRegressor(colsample_bytree = 0.7)\n",
    "        param_grid = {\n",
    "            'max_depth': [3,4,5],\n",
    "            'learning_rate': [0.01, 0.3, 0.1],\n",
    "            'gamma': [0.2, 0.6, 1],\n",
    "            'reg_lambda': [0, 0.1, 1, 10, 100]\n",
    "        }\n",
    "        cv = TimeSeriesSplit(n_splits=7)\n",
    "        return GridSearchCV(estimator = xgb.XGBRegressor(colsample_bytree = 0.7), param_grid = param_grid, cv = cv)\n",
    "    if methode == \"knn\":\n",
    "        model = KNeighborsRegressor()\n",
    "        cv = TimeSeriesSplit(n_splits=5)\n",
    "        grid = {\n",
    "            'n_neighbors' : [2,3,5,7,11,17,23],\n",
    "            'weights' : ['uniform', 'distance']\n",
    "        }\n",
    "        return GridSearchCV(model, grid, cv = cv)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6888038",
   "metadata": {
    "id": "d6888038"
   },
   "source": [
    "## Fonctions pour le Shapley de Coalitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e79743",
   "metadata": {
    "id": "a5e79743"
   },
   "outputs": [],
   "source": [
    "def XtoB(X, methode = \"moyenne\"):\n",
    "    if methode == \"moyenne\":\n",
    "        return X.mean()\n",
    "    elif methode == \"zero\":\n",
    "        return np.zeros(X.shape)\n",
    "#Transforme une liste de coalitions d'ordre supérieur à 1 en une liste de coalition de tout ordre (non nul)\n",
    "def CtoC(L, n):\n",
    "    Out = L\n",
    "    Coaled = []\n",
    "    for l in L:\n",
    "        Coaled += l\n",
    "    for i in range(n):\n",
    "        if i not in Coaled:\n",
    "            Out.append([i])\n",
    "    return Out\n",
    "\n",
    "#Transforme une liste d'indice I en un vecteur approchant une observation X, relativement à une partition C,\n",
    "# une coalition C[k] et une référence B\n",
    "def ChoixVar(X, B, C, I, k):        \n",
    "    X_ = np.zeros(len(X))\n",
    "    for i in range(len(I)):\n",
    "        for j in C[i]:\n",
    "            X_[j] = X[j]*I[i] + B[j]*(1-I[i])\n",
    "    X__ = np.copy(X_)\n",
    "    for j in C[k]:\n",
    "        X_[j] = X[j]\n",
    "        X__[j] = B[j]\n",
    "    return X_, X__\n",
    "#Calcul des Shapleys, retourne une liste de Shapleys pour une fonction de coût f, une observation X,\n",
    "#une référence B et une partition C\n",
    "#Q et M sont des paramètres de complexité\n",
    "def Compute(Q, M, f, X, B = np.zeros(200), C = []):\n",
    "    n = len(X)\n",
    "    Coalitions = CtoC(C, n)\n",
    "    nc = len(Coalitions)\n",
    "    S = np.zeros(nc)\n",
    "    #Première boucle: estimation de l'intégrale\n",
    "    for q in trange(Q//2):\n",
    "        E = np.zeros(nc)\n",
    "        #Deuxième boucle: estimation de l'espérance par méthode de Monte-Carlo\n",
    "        for m in range(M):\n",
    "            # Echantillonage\n",
    "            I = bernoulli.rvs(q/Q, size = nc)\n",
    "            Ineg = 1-I\n",
    "            for k in range(nc):\n",
    "                X_, X__ = ChoixVar(X, B, Coalitions, I, k)\n",
    "                hk = float(f(X_.reshape(1, -1)) - f(X__.reshape(1, -1)))\n",
    "                # On utilise un même échantillonage pour p = q et p = 1-q pour réduire le temps de calcul\n",
    "                X_neg, X__neg = ChoixVar(X, B, Coalitions, Ineg, k)\n",
    "                hk_neg = float(f(X_neg.reshape(1, -1)) - f(X__neg.reshape(1, -1)))\n",
    "                E[k] = E[k] + hk + hk_neg\n",
    "        for k in range(nc):\n",
    "            S[k] = S[k] + E[k]\n",
    "    # Nom des coalitions\n",
    "    I = []\n",
    "    try:\n",
    "        Cnames = list(X.index)\n",
    "        for l in Coalitions:\n",
    "            I.append(str([Cnames[m] for m in l]))\n",
    "    except AttributeError:\n",
    "        for l in Coalitions:\n",
    "            I.append(str([m for m in l]))\n",
    "    Shapley = pd.Series(S/(Q*M), index = I)\n",
    "    return Shapley"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e728936",
   "metadata": {
    "id": "0e728936"
   },
   "source": [
    "## Exemple de réalisation de prédictions à horizon 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6069c4a7",
   "metadata": {
    "id": "6069c4a7",
    "outputId": "4ebc9736-83f8-4795-f9f3-03f10eabde8a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Définition d'un cluster de produits\n",
    "Cluster1 = [70181, 70208, 60887, 70212, 70214, 70215, 70731, 60257, 69776, 60262, 66589, 67452, 62850, 67849, 69496, 65652, 63837, 60890, 66594, 60741, 63701, 62458, 70197, 69728, 63921, 65649, 70199, 62461, 63922]\n",
    "# Formatage de la liste\n",
    "C1 = list_to_list(Cluster1)\n",
    "# Préparation des données\n",
    "Xtrain, Ytrain, Xtest, Ytest = preprocess_list(C1, C1[0])\n",
    "# application d'un modèle\n",
    "reg = get_a_model(\"xgb\")\n",
    "reg.fit(Xtrain, Ytrain)\n",
    "\n",
    "Yhat = reg.best_estimator_.predict(Xtest)\n",
    "# Résultat\n",
    "Data = pd.DataFrame({\"Yhat\":Yhat[1:], \"Ytest\":Ytest.values[:-1]}, index =Ytest.index[:-1])\n",
    "Data.plot()\n",
    "rmse(Yhat[1:],Ytest.values[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5de8563",
   "metadata": {
    "id": "e5de8563"
   },
   "source": [
    "## Exemple de réalisation de prédictions à horizon >1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee45d8d",
   "metadata": {
    "id": "eee45d8d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ne convient qu'aux modèles univariés, mais pourait être adapté pour inclure les variables connues à l'avance\n",
    "def predHorizon(Data, f, h = 14):\n",
    "    A = [Data.values[i][0] for i in range(len(Data.values)-60,len(Data.values))]\n",
    "    for i in range(h):\n",
    "        Y = f(A[i:])\n",
    "        A.append(Y)\n",
    "    return A\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = preprocess_list(C1, C1[0], Exo = False)\n",
    "S_train, S_test = preprocess_(indice_to_df(C1[0]), Out = 14, In = 60, Exo = False, Cat = False)\n",
    "regUniv = get_a_model(\"xgb\")\n",
    "regUniv.fit(X_train, Y_train)\n",
    "def freg(S):\n",
    "    return regUniv.best_estimator_.predict(np.array(S).reshape(1,-1))[0]\n",
    "Pred = predHorizon(S_train, freg)[-14:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc476bbb",
   "metadata": {
    "id": "fc476bbb",
    "outputId": "0c828887-07c6-4a69-ef9c-0cd4e26ab119"
   },
   "outputs": [],
   "source": [
    "Data = pd.DataFrame({\"Yhat\":Pred, \"Ytest\":Ytest.values}, index =Ytest.index)\n",
    "Data.plot()\n",
    "rmse(Pred, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275dd5ac",
   "metadata": {
    "id": "275dd5ac"
   },
   "source": [
    "## Mesure d'importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106b7e15",
   "metadata": {
    "id": "106b7e15",
    "outputId": "b3a6efde-9f1d-4d0f-ad1a-0aae8e137e45"
   },
   "outputs": [],
   "source": [
    "#Mesure d'importance de XGBoost\n",
    "Coefs = abs(pd.DataFrame(reg.best_estimator_.feature_importances_))\n",
    "Index = []\n",
    "for i in range(10):\n",
    "    if Coefs.max()[0] >0:\n",
    "        Index.append(int(Coefs.idxmax()))\n",
    "        Coefs[0][Index[i]] = 0\n",
    "Xtrain.columns[Index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172a6359",
   "metadata": {
    "id": "172a6359",
    "outputId": "f7bb7ddd-518e-4d44-817f-77672cb23f12"
   },
   "outputs": [],
   "source": [
    "#Bibliothèque SHAP\n",
    "explainer = shap.Explainer(reg.best_estimator_.predict, Xtrain)\n",
    "shap_values = explainer(Xtest)\n",
    "shap.plots.bar(shap_values, max_display=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32347cc4",
   "metadata": {
    "id": "32347cc4",
    "outputId": "49ffe24d-f37c-48a6-9e06-d975f502ab71",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Shapley de coalitions\n",
    "Coalitions = [[0,13,14,15],[1,2,3,7,8,10],[5,6],[9,11,16,17,18,19,20, 21], [22,23,24, 25],list(range(21+5,21+49)), [21+49, 21+50, 21+51, 21+52, 21+53, 21+54], list(range(21+55, 21+61))]\n",
    "\n",
    "Compute(20, 10, reg.best_estimator_.predict, Xtest.iloc[2], Xtrain.mean(), Coalitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf2c325",
   "metadata": {
    "id": "1bf2c325"
   },
   "source": [
    "### Exemple d'application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8378ad",
   "metadata": {
    "id": "9e8378ad"
   },
   "outputs": [],
   "source": [
    "Filtre = ['ordered_volumes-55', 'ordered_volumes-56', 'ordered_volumes-57', 'ordered_volumes-58', 'ordered_volumes-59', 'ordered_volumes-60', 'promo_uplift_coefficient', 'apparenttemperaturemax', 'apparenttemperaturemax-1', 'apparenttemperaturemax-2', 'apparenttemperaturemax-3', 'ordered_volumes-22', 'ordered_volumes-23', 'ordered_volumes-24', 'ordered_volumes-25', 'ordered_volumes-26', 'ordered_volumes-27', 'ordered_volumes-28', 'ordered_volumes-29', 'ordered_volumes-30', 'ordered_volumes-31', 'ordered_volumes-32', 'ordered_volumes-33', 'ordered_volumes-34']\n",
    "Xtrain, Ytrain, Xtest, Ytest = preprocess_list(C1, C1[0], Filtre = Filtre)\n",
    "\n",
    "regFiltre = get_a_model(\"xgb\")\n",
    "regFiltre.fit(Xtrain, Ytrain)\n",
    "Yhat = reg.best_estimator_.predict(Xtest)\n",
    "# Résultat\n",
    "Data = pd.DataFrame({\"Yhat\":Yhat[1:], \"Ytest\":Ytest.values[:-1]}, index =Ytest.index[:-1])\n",
    "Data.plot()\n",
    "rmse(Yhat[1:],Ytest.values[:-1])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
