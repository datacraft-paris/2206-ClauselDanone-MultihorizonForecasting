{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6126a6c9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "   <center>\n",
    "       <h3>Time Series Multi-Horizon Forecasting : comment prédire des phénomènes non stationnaires à des horizons de temps multiples ?</h3>\n",
    "       <br>\n",
    "      <p>Bonjour et bienvenue à cet atelier datacraft en collaboration avec Danone.</p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f0b343",
   "metadata": {},
   "source": [
    "L'objectif de cet atelier est de prédire la colonne ``ordered_volumes`` du jeu de données à différents horizons (1 semaine, 3 mois, 1 an).\n",
    "\n",
    "Il y a au total 125 produits différents mais vous pouvez vous concentrer sur seulement un produit de chaque cluster déterminé par Gabriel. Cette partie est détaillée dans le notebook **Clustering datacraft**\n",
    "\n",
    "Nous pouvons donc nous concentrer sur un ou deux produit(s) par clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d854bbe2",
   "metadata": {},
   "source": [
    "Ce notebook est dédié à l'exploration des données Danone et à l'utilisation des packages **prophet** et **neural prophet**.\n",
    "\n",
    "La fonction ``train_prophet_model`` permet d'entrainer un model prophet tout en modulant les paramètres en se passant de l'API prophet d'origine.\n",
    "\n",
    "La documentation et les exemples permettent de bien comprendre son utilisation mais n'hésitez pas à nous poser la moindre question.\n",
    "\n",
    "Si vous voulez vous passer de la fonction et passer directement par l'API (notamment si vous voulez faire de la cross validation, ajouter des saisonnalités ou moduler des paramètres non présents dans la fonction) je vous invite à regarder la doc prophet : \n",
    "\n",
    "- https://facebook.github.io/prophet/docs/quick_start.html#python-api\n",
    "- https://github.com/facebook/prophet/blob/main/python/prophet/forecaster.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845dca74",
   "metadata": {},
   "source": [
    "**Mémo** : \n",
    "\n",
    "<ul> \n",
    "    <li> Avancer sur :\n",
    "        <ul>  \n",
    "            <li>Coupler prophet avec XGBoost à partir des résidus</li>\n",
    "            <li>Tenter la <a href=\"https://github.com/gpeyre/numerical-tours/blob/master/python/ml_11_conformal_prediction.ipynb\">conformal prediction</a> (interval de confiance)</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li> Questions danone :\n",
    "        <ul>\n",
    "            <li> demander a danone la promo : promo sur le prix ? quel quantité ? prix ou pourcent ou volume ? \n",
    "                horizon 0 = date courante ? </li>\n",
    "            <li> Est-ce que la préd par produit est plus pertinente que la pred par cluster ou pour la totalité des produits ?Indépendance de commande entre les produits ou non ? </li>\n",
    "        </ul>   \n",
    "    </li>\n",
    "    <li>\n",
    "        Améliorations :\n",
    "        <ul>\n",
    "            <li>Retirer les produits qui ne sont plus commercialisés et ceux qui ne sont pas dans la fenetre temporelle choisie ?\n",
    "    Pour avoir une timeline uniforme</li>\n",
    "            <li> Dashboard => Ne pas fonctionner par ligne mais : plusieurs produit avec une data de sép et plusieurs périodes éventuellement</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205fc909",
   "metadata": {},
   "source": [
    "# Charger les librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969108cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prophet\n",
    "import numpy as np\n",
    "from dash import Dash, State\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from prophet import Prophet \n",
    "from prophet.plot import add_changepoints_to_plot\n",
    "from neuralprophet import NeuralProphet \n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.tools import mpl_to_plotly\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dash.exceptions import PreventUpdate\n",
    "\n",
    "from jupyter_dash import JupyterDash\n",
    "from dash import dcc\n",
    "from dash import html\n",
    "from dash.dependencies import Output, Input\n",
    "\n",
    "from dash import html\n",
    "import dash_bootstrap_components as dbc\n",
    "\n",
    "from statsmodels.graphics.gofplots import qqplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebc6728",
   "metadata": {},
   "source": [
    "# Charger les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad67b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"./data/forecasting.parquet.gzip\")\n",
    "danone = df.copy(deep = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cef404e",
   "metadata": {},
   "source": [
    "# Dictionnaire de sélection de variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0763d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Colonnes qui vont rester dans le dataframe ou non, 0 si non, 1 si oui, \n",
    "si le nom n'est pas dedans, alors la colonne ne reste pas'''\n",
    "\n",
    "selected_columns = {\"time_index\" : 1,\"apparenttemperaturemax\" :1,\"cos_iso_month\" : 0,\"cos_iso_week\" : 0,\"cos_iso_week_of_month\": 0,\"days_before_next_holiday\": 0,\"forecasted_volumes\": 0,\"fu_cod\": 0,\"future_ordered_volumes_until_saturday_of_current_week\": 0,\"future_ordered_volumes_until_saturday_of_previous_week\": 0,\"holiday_day_of_week\": 0,\"holidays_count_in_week\": 0,\"iso_month\": 0,\"iso_week\": 0,\"iso_week_of_month\": 0,\"mat_net_weight_value_kg\": 0,\"ordered_volumes\": 1,\"precipintensity\": 1,\"promo_uplift_coefficient\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fe62a5",
   "metadata": {},
   "source": [
    "# Présentation du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a468ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values([\"product_index\", \"time_index\"])\\\n",
    "    .head()[[\"time_index\",\"product_index\",\"ordered_volumes\", \"future_ordered_volumes_until_saturday_of_current_week\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72afa58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values([\"product_index\",\"time_index\"])\\\n",
    "    .head()[[\"time_index\",\"product_index\",\"ordered_volumes\", \"promo_mean_horizon_0\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dea284",
   "metadata": {},
   "source": [
    "Dans ce dataset, on a des données sur le volume des commandes réalisés par les clients, le tout par semaine.\n",
    "On aura le total des commandes effectuées par les clients du Lundi au Vendredi, cette données est disponible dans les colonnes : <br> <b>ordered_volumes</b> ainsi que dans <br> <b>future_ordered_volumes_until_saturday_of_current_week</b>\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5bfd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values([\"product_index\", \"time_index\"])\\\n",
    "    .head()[[\"time_index\",\"product_index\",\"precipintensity\", \"apparenttemperaturemax\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22429373",
   "metadata": {},
   "source": [
    "Ici, nous avons affichés certains <b>régresseurs</b>, ils permettent de rajouter de l'information sur la timeline actuelle et éventuellement d'expliquer certains changements aperçus sur la colonne du volume de commandes.\n",
    "Le modèle <b>prophet</b> ira regarder ces variables et pourra en fonction de la corrélation appliquer des corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96fe38f",
   "metadata": {},
   "source": [
    "## Fonctions utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358ca2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale_df(df):\n",
    "    \"\"\"\n",
    "        Allows to transform data with large scale into percent of min-max\n",
    "        from data set\n",
    "\n",
    "        @param : df, dataframe\n",
    "        @return : df, dataframe\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    df[\"ordered_volumes\"] = scaler.fit_transform(np.array(df[\"ordered_volumes\"]).reshape(-1,1))\n",
    "    return df\n",
    "\n",
    "def get_df_selected_columns(df,selected_columns):\n",
    "    \"\"\"\n",
    "        Allows to select columns of a dataframe \n",
    "\n",
    "        @param : df, dataframe\n",
    "        @return : df, dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = []\n",
    "    for col in selected_columns:\n",
    "        if selected_columns[col] == 1:\n",
    "            columns.append(col)\n",
    "    \n",
    "    return df[columns]\n",
    "\n",
    "def year_to_date(date_sep):\n",
    "    \"\"\"\n",
    "        Convert year (from int format) to date based on 1 January <year>\n",
    "        \n",
    "        @param : year, int\n",
    "        @return : date, converted year to date\n",
    "    \"\"\"\n",
    "    if type(date_sep) in [float, int]:\n",
    "        date_sep = datetime.strptime(str(date_sep), \"%Y\")\n",
    "\n",
    "    else:\n",
    "        if len(date_sep) == 4:\n",
    "            date_sep = datetime.strptime(str(date_sep), \"%Y\")\n",
    "        else :\n",
    "            date_sep = datetime.strptime(str(date_sep), \"%Y-%m-%d\")\n",
    "    return date_sep\n",
    "    \n",
    "\n",
    "def prepare_df(df,product_id,selected_columns):\n",
    "    \n",
    "    \"\"\"\n",
    "        Processings on df : \n",
    "            - Select wanted product\n",
    "            - Sort from start date time to end\n",
    "            - Distinguish necessary columns from regressors\n",
    "            - Rename columns (df,y) to fit prophet model expectations\n",
    "        \n",
    "        @param : df, dataframe\n",
    "        @return : df (dataframe), regressor (list of regressors column's names)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.query(f'product_index=={product_id}')\n",
    "    df = get_df_selected_columns(df,selected_columns).sort_values('time_index')\n",
    "    updated_columns = set(df.columns)\n",
    "    main_columns = set([\"time_index\", \"ordered_volumes\"])\n",
    "    \n",
    "    regressor = updated_columns.difference(main_columns)\n",
    "    \n",
    "    df= df[df['ordered_volumes']>0]\n",
    "    df.rename(columns = {\"time_index\" : \"ds\", \"ordered_volumes\" : \"y\"}, inplace=True)\n",
    "    df.dropna(subset=regressor, inplace=True)\n",
    "    regressor = list(regressor)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    df.dropna(subset=regressor, inplace=True)\n",
    "    \n",
    "    \n",
    "    return df, regressor\n",
    "    \n",
    "def split_df(df, prediction_w_period,separation_date):\n",
    "    \n",
    "    \"\"\"\n",
    "        Used to make train set and test set, takes a separation date\n",
    "        and a prediction period in weeks\n",
    "        \n",
    "        @param : df, dataframe\n",
    "        @param : prediction_w_period, period of prediction in weeks\n",
    "        @param : separation_date, the separation date which takes\n",
    "        the first part for train and second part calculated with prediction_w_period\n",
    "    \"\"\"\n",
    "    \n",
    "    separation_date = year_to_date(separation_date)\n",
    "    \n",
    "    df_train = df[df.ds.apply(lambda date:date)<separation_date]\n",
    "    df_test = df[df.ds.apply(lambda date:date)>=separation_date]\n",
    "    df_test = df_test.iloc[:prediction_w_period]\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "def get_tensors_quantile_losses(forecast, danone):\n",
    "    tensPred = torch.from_numpy(np.asarray([forecast[\"yhat\"].to_list()]))\n",
    "    tensReal = torch.from_numpy(np.asarray([danone[\"y\"].to_list()]))\n",
    "    losses = QuantileLoss(quantiles = [0.1,0.5,0.9]).loss(tensPred,tensReal)\n",
    "    k=0\n",
    "    for i in losses:\n",
    "        for j in i :\n",
    "            print(j)\n",
    "            print(forecast[\"yhat\"].to_list()[k])\n",
    "            print(danone[\"y\"].to_list()[k])\n",
    "            print(k)\n",
    "            k += 1\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7681fdc7",
   "metadata": {},
   "source": [
    "Est-ce que la préd par produit est plus pertinente que la pred par cluster ou pour la totalité des produits ?\n",
    "Indépendance de commande entre les produits ou non ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d082282",
   "metadata": {},
   "source": [
    "# Voir la documentation : <a href = \"https://github.com/ourownstory/neural_prophet/blob/main/neuralprophet/forecaster.py\">NeuralProphet</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9526b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralParams = {\n",
    "    \"changepoints\" : None, #lorsqu'on donne ce paramètre, le model ne va detecter aucun changepoints (ce n'est pas le paramètre par défaut)\n",
    "    \"n_changepoints\" : 25, # nombre de changepoints # pas utile si on spécifie changepoints\n",
    "    \"changepoints_range\" : 0.8, # n_changepoints répartis sur 80% sur train set \n",
    "    \"trend_reg\" : 0,\n",
    "    \"trend_reg_threshold\" : False,\n",
    "    \"yearly_seasonality\" :\"auto\",\n",
    "    \"weekly_seasonality\" :\"auto\",\n",
    "    \"daily_seasonality\": \"auto\",\n",
    "    \"seasonality_mode\" : \"additive\",\n",
    "    \"seasonality_reg\" : 0,\n",
    "    \"n_forecasts\" : 1,\n",
    "    \"n_lags\" : 0,\n",
    "    \"num_hidden_layers\" : 0,\n",
    "    \"d_hidden\" : None,\n",
    "    \"ar_reg\" : None,\n",
    "    \"learning_rate\" : None,\n",
    "    \"epochs\" : None,\n",
    "    \"batch_size\" : None,\n",
    "    \"loss_func\" : \"Huber\",\n",
    "    \"optimizer\" : \"AdamW\",\n",
    "    \"newer_samples_weight\" : 2,\n",
    "    \"newer_samples_start\" : 0.0,\n",
    "    \"impute_missing\" : True,\n",
    "    \"collect_metrics\" :True,\n",
    "    \"normalize\" :\"auto\",\n",
    "    \"global_normalization\" :False,\n",
    "    \"global_time_normalization\" : True,\n",
    "    \"unknown_data_normalization\" : False,\n",
    "}\n",
    "\n",
    "def train_neural_prophet_model( params = neuralParams,\n",
    "                                df = danone,\n",
    "                                freq = \"W-MON\",\n",
    "                                product_id = 70189,\n",
    "                                date_sep = 2021,\n",
    "                                periode = 12 # nombre de semaines dans le test_set\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    Process the danone df, split it, train the model on the train. \n",
    "\n",
    "    Return the trained neural model, the processed prophet friendly df, the forecast df with forecasted values and the residuals df with residuals.\n",
    "    \n",
    "    @param params : dict, hyper-parameters for neural prophet model configuration\n",
    "    \n",
    "    @param df : dataframe\n",
    "    \n",
    "    @param freq : string, frequency in the dataframe\n",
    "    \n",
    "    @param product_id : int, the product id\n",
    "    \n",
    "    @param date_sep : int or date, date of separation for train and test set (2017 < date_sep < 2022)\n",
    "    \n",
    "    @param periode : int, number of weeks that we want to predict (test set)\n",
    "    \n",
    "    @return m : prophet model : the trained model\n",
    "    \n",
    "    @return df : DataFrame : the processed df in a prophet-friendly shape \n",
    "    \n",
    "    @return future : DataFrame : a single column DataFrame with train set dates and test set dates concatenated\n",
    "    \n",
    "    @return residuals : DataFrame with e = y-yhat for each prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    selected_columns = {\"time_index\" : 1, \"ordered_volumes\" : 1}\n",
    "    \n",
    "    df, regressor = prepare_df(df, product_id, selected_columns)\n",
    "    df_train, df_test = split_df(df,periode,date_sep)\n",
    "    date_sep = year_to_date(date_sep)\n",
    "            \n",
    "    #if add_holiday == True: \n",
    "    \n",
    "            \n",
    "    m = NeuralProphet(**params)\n",
    "    \n",
    "    m.add_country_holidays(\"FR\")\n",
    "    \n",
    "    df_train, df_val = NeuralProphet().split_df(df_train, valid_p=0.2)\n",
    "    \n",
    "    m.fit(df_train, freq = freq, validation_df = df_val)\n",
    "    \n",
    "    forecast = m.predict(df_test)\n",
    "    \n",
    "    #metrics = m.test(df_test)\n",
    "    \n",
    "    future = df_test\n",
    "    \n",
    "    df = pd.concat([df_train, df_val],axis=0)\n",
    "    \n",
    "    df = pd.concat([df, df_test], axis=0)\n",
    "    \n",
    "    residuals = pd.DataFrame()\n",
    "    \n",
    "    residuals['ds']=df[df.ds>=date_sep][\"ds\"]\n",
    "\n",
    "    residuals['e'] = df[df.ds>=date_sep]['y']-forecast[forecast.ds>=date_sep]['yhat1']\n",
    "\n",
    "    residuals.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return m, df, future, forecast, residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfde795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "            \"growth\" : 'linear', #type de trend 'linear', 'logistic' or 'flat',si growth = logistic on est obligé de renseigné cap \n",
    "            \"changepoints\" : None, #lorsqu'on donne ce paramètre, le model ne va detecter aucun changepoints (ce n'est pas le paramètre par défaut)\n",
    "            \"n_changepoints\" : 25, #nombre de changepoints # pas utile si on spécifie changepoints\n",
    "            \"changepoint_range\" : 0.8, #n_changepoints répartis sur 80% sur train set\n",
    "            \"yearly_seasonality\" : 'auto', # un entier sinon \n",
    "                                    #The default values are often appropriate, but they can be increased when the seasonality needs to fit higher-frequency changes, and generally be less smooth\n",
    "                                    # tester 'auto' et 10 qui est selon la doc la valeur par défaut\n",
    "            \"weekly_seasonality\" : 'auto',\n",
    "            \"daily_seasonality\" : 'auto',\n",
    "            \"holidays\" : None,\n",
    "            \"seasonality_mode\" : 'additive', #ou mutliplicative\n",
    "            \"seasonality_prior_scale\" : 10.0, # si les vacances ou les saisonnalités sont overfittés ou peut utiliser prior scale (defaut =10)\n",
    "            \"holidays_prior_scale\" : 10.0,\n",
    "            \"changepoint_prior_scale\" : 0.05, #sensibilité aux changement dans la trend (si on estime par exemple que le changement de trend n'en est pas un)\n",
    "            \"mcmc_samples\": 0,\n",
    "            \"interval_width\" : 0.80,\n",
    "            \"uncertainty_samples\": 1000,\n",
    "            \"stan_backend\": None\n",
    "}\n",
    "\n",
    "def train_prophet_model(df = danone,\n",
    "                params = params,\n",
    "                product_id = 70189,\n",
    "                        \n",
    "                add_holiday = True,\n",
    "\n",
    "                regressor = [], \n",
    "                cap = None, # default = None# carrying capacity : When forecasting growth, there is usually some maximum achievable point: total market size = carrying capacity\n",
    "                        #cap peut être une liste comme une constante\n",
    "\n",
    "                floor = None,\n",
    "                \n",
    "                date_sep = 2021,\n",
    "\n",
    "                periode = 12 # nombre de semaine dans le test_set\n",
    "):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Process the danone df, split it, train the model on the train. \n",
    "\n",
    "    Return the trained model, the processed prophet friendly df, the forecast df with forecasted values and the residuals df with residuals.\n",
    "        \n",
    "    @param df, DataFrame (default=danone)\n",
    "    \n",
    "    @param product_id, int : the produt_index we want to forecast, must be in the 'product_index' column of df\n",
    "    \n",
    "    @param date_sep int or string : the date at which we want to divide our dataset. \n",
    "    example : '2019', 2019, '2020-01-13'\n",
    "    \n",
    "    @param periode, int : number of weeks we want in the train set \n",
    "    \n",
    "    @param cap : int, DataFrame, Series, array, list : carrying capacity : When forecasting growth, there is usually some maximum achievable point: total market size, total population size, etc. This is called the carrying capacity, and the forecast should saturate at this point. \n",
    "    \n",
    "    @param floor : int, DataFrame, Series, array, list saturating minimum\n",
    "    \n",
    "    @param interval : Float, width of the uncertainty intervals provided for the forecast.\n",
    "    If mcmc_samples=0, this will be only the uncertainty in the trend using the MAP estimate of the extrapolated generative model.\n",
    "    If mcmc.samples>0, this will be integrated over all model parameters, which will include uncertainty in seasonality. \n",
    "    \n",
    "    @param add_holidays, bool : True if we want to include the effects of prophet built-in holidays (defalut=False)\n",
    "    (Jour de l'an, Fête du Travail, Armistice 1945, Fête nationale, Armistice 1918, Lundi de Pâques, Lundi de Pentecôte, Ascension, Assomption, Toussaint, Noël)\n",
    "    \n",
    "    @param regressor, list : list of the regressors we want to include in the model, must be names of columns of df. (default=empty list)\n",
    "    \n",
    "    @param changepoints, list : list of dates where we suspect a change in the trend, dates must be strings and must be in the time_index column of df (default=None)\n",
    "    \n",
    "    @param n_changepoints, int : if changepoints=None, the model distribute n_changepoints on the train_set and test if there is a change in the trend at those points.\n",
    "    Not used if `changepoints` is specified\n",
    "    \n",
    "    @param changepoint_range, float in [0,1] : Proportion of history in which trend changepoints willbe estimated.\n",
    "    Defaults to 0.8 for the first 80%. Not used if`changepoints` is specified\n",
    "    \n",
    "    @param yearly_seasonality : 'auto', True, False or a number of Fourier terms to generate (default=auto)\n",
    "    \n",
    "    \n",
    "    @param holidays : DataFrame with columns holiday (string) and ds (date type)\n",
    "    and optionally columns lower_window and upper_window which specify arange of days around the date to be included as holidays.lower_window=-2 will include 2 days prior to the date as holidays.\n",
    "    Also optionally can have a column prior_scale specifying the prior scale for that holiday.\n",
    "    \n",
    "    @param seasonality_mode : 'additive'(default), 'mutltiplicative'\n",
    "    \n",
    "    @param seasonality_prior_scale : int or float, Parameter modulating the strength of the seasonality model.\n",
    "    Larger values allow the model to fit larger seasonal fluctuations, smaller values dampen the seasonality. Can be specified for individual seasonalities using add_seasonality.\n",
    "    \n",
    "    @param holidays_prior_scale : int or float, Parameter modulating the strength of the holiday components model, unless overridden in the holidays input.\n",
    "    \n",
    "    @param mcmc_samples : int, if greater than 0, will do full Bayesian inference with the specified number of MCMC samples. If 0, will do MAP estimation.\n",
    "    \n",
    "    @param uncertainty_samples : Number of simulated draws used to estimate uncertainty intervals. Settings this value to 0 or False will disable uncertainty estimation and speed up the calculation. \n",
    "    \n",
    "    @return m : prophet model : the trained model\n",
    "    \n",
    "    @return df : DataFrame : the processed df in a prophet-friendly shape \n",
    "    \n",
    "    @return future : DataFrame : a single column DataFrame with train set dates and test set dates concatenated\n",
    "    \n",
    "    @return residuals : DataFrame : DataFrame : DataFrame with e = y-yhat for each prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.query(f'product_index=={product_id}')\n",
    "    df = df[['time_index','ordered_volumes']+regressor].sort_values('time_index')\n",
    "    df = df[df['ordered_volumes']>0]\n",
    "    df.columns = ['ds','y']+regressor\n",
    "    df.dropna(subset=regressor, inplace=True)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "    if cap !=None and growth == 'linear':\n",
    "        df['cap'] = cap ####################### future a remplacer \n",
    "        df['floor'] = floor\n",
    "\n",
    "    if floor !=None and growth == 'logistic':\n",
    "        df['cap'] = cap \n",
    "        df['floor'] = floor  #####################\n",
    "\n",
    "    df.dropna(subset=regressor, inplace=True)\n",
    "\n",
    "\n",
    "    if type(date_sep) in [float, int]:\n",
    "        date_sep = datetime.strptime(str(date_sep), \"%Y\")\n",
    "        df_train = df[df.ds<date_sep]\n",
    "        df_test = df[df.ds>=date_sep]\n",
    "\n",
    "    else:\n",
    "        if len(date_sep) == 4:\n",
    "            date_sep = datetime.strptime(str(date_sep), \"%Y\")\n",
    "            df_train = df[df.ds<date_sep]\n",
    "            df_test = df[df.ds>=date_sep]\n",
    "        else :\n",
    "            date_sep = datetime.strptime(str(date_sep), \"%Y-%m-%d\")\n",
    "            df_train = df[df.ds<date_sep]\n",
    "            df_test = df[df.ds>=date_sep]\n",
    "\n",
    "    m = Prophet(**params)\n",
    "\n",
    "    if add_holiday == True: \n",
    "        m.add_country_holidays(country_name=\"FR\")\n",
    "\n",
    "    for reg in regressor:\n",
    "        m.add_regressor(regressor)\n",
    "\n",
    "    m.fit(df_train)\n",
    "\n",
    "    future = m.make_future_dataframe(periods=len(df_test), freq='W-MON', include_history=True)\n",
    "\n",
    "    future[regressor] = df[regressor] # à modifier si include_history=False\n",
    "\n",
    "    if cap !=None and growth == 'linear':\n",
    "        future['cap'] = cap  \n",
    "\n",
    "    if floor !=None and growth == 'logistic':\n",
    "        future['cap'] = cap \n",
    "        future['floor'] = floor \n",
    "\n",
    "    forecast = m.predict(future)\n",
    "\n",
    "    residuals = pd.DataFrame()\n",
    "    \n",
    "    df = pd.concat([df_train,df_test], axis = 0)\n",
    "\n",
    "    residuals['ds']=future.tail(periode)[\"ds\"]\n",
    "\n",
    "    residuals['e'] = df[df.ds>=date_sep]['y']-forecast[forecast.ds>=date_sep]['yhat']\n",
    "\n",
    "    residuals.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return m, df, future, forecast, residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aba555",
   "metadata": {},
   "source": [
    "# Affichage des performances du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af55623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_my_prophet_graph(forecast, product_id = 70189,):\n",
    "    \n",
    "    \"\"\"\n",
    "    Show model's benchmarks via graphical interface\n",
    "    \n",
    "    @param : forecast, dataframe with ds, y and yhat (prediction value)\n",
    "    @param : product_id, the product id to select\n",
    "    @param : year, the separation year between train and test set\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    mse = mean_squared_error(forecast['yhat'],forecast['y'])\n",
    "    mae = mean_absolute_error(forecast['yhat'],forecast['y'])\n",
    "\n",
    "    s1 = go.Scatter(x=forecast['ds'], y=forecast['yhat_lower'], name='yhat_lower',fill='tonexty',line={\"color\":\"gray\"},fillcolor='rgba(68, 68, 68, 0.1)',showlegend=True)\n",
    "\n",
    "    s2 = go.Scatter(x=forecast['ds'], y=forecast['yhat'], name='prediction',line={\"color\":\"red\"})\n",
    "\n",
    "    s3 = go.Scatter(x=forecast['ds'], y=forecast['yhat_upper'], name='yhat_upper',line={\"color\":\"gray\"},showlegend=True)\n",
    "\n",
    "    fig = go.Figure(data=[s2,s3,s1],layout={'title':f'Prédiction à 1 an des quantités de {product_id}'})\n",
    "\n",
    "    fig.add_scatter(x=df['ds'],y=forecast['y'],mode='markers',name='Quantité observée')\n",
    "\n",
    "    #fig.add_vline(x=str(year_to_date(year)), line_width=3, line_color=\"green\")\n",
    "\n",
    "    fig.add_annotation(x='2017', y=600,\n",
    "                text=f\"MSE : {mse}\",\n",
    "                showarrow=False,\n",
    "                arrowhead=1)\n",
    "\n",
    "    fig.add_annotation(x='2017', y=550,\n",
    "                text=f\"MAE : {mae}\",\n",
    "                showarrow=False,\n",
    "                arrowhead=1)\n",
    "\n",
    "\n",
    "    fig.update_layout(hovermode='x',\n",
    "                     xaxis_title='Date',yaxis_title='Quantités')\n",
    "    \n",
    "    return(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14798314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_my_neural_graph(forecasts, product_id = 70189):\n",
    "    \"\"\"\n",
    "    Show model's benchmarks via graphical interface\n",
    "    \n",
    "    @param : forecast, dataframe with ds, y and yhat (prediction value)\n",
    "    @param : product_id, the product id to select\n",
    "    @param : year, the separation year between train and test set\n",
    "    \"\"\"\n",
    "        \n",
    "    #mse = mean_squared_error(forecast['yhat1'],forecast['y']) ######### a modifier\n",
    "    #mae = mean_absolute_error(forecast['yhat1'],forecast['y'])\n",
    "    \n",
    "    dat = []\n",
    "    \n",
    "    for forecast in forecasts :\n",
    "        rd = random.randint(0,16777215)\n",
    "        hex_number = str(hex(rd))\n",
    "        hex_color ='#'+ hex_number[2:]\n",
    "        dat.append(go.Scatter(x=forecast['ds'], y=forecast['yhat1'], name='prediction',line={\"color\":hex_color}))\n",
    "        \n",
    "    fig = go.Figure(data=dat,layout={'title':f'Prédiction à 1 an des quantités de {product_id}'})\n",
    "    \n",
    "    for forecast in forecasts :\n",
    "        fig.add_scatter(x=forecast['ds'],y=forecast['y'],mode='markers',name='Quantité observée')\n",
    "        '''fig.add_annotation(x='2017', y=600,\n",
    "                text=f\"MSE : {mean_squared_error(forecast['yhat1'],forecast['y'])}\",\n",
    "                showarrow=False,\n",
    "                arrowhead=1)\n",
    "        fig.add_annotation(x='2017', y=550,\n",
    "                text=f\"MAE : {mean_absolute_error(forecast['yhat1'],forecast['y'])}\",\n",
    "                showarrow=False,\n",
    "                arrowhead=1)'''\n",
    "        \n",
    "\n",
    "    #fig.add_vline(x=str(year_to_date(year)), line_width=3, line_color=\"green\")\n",
    "\n",
    "    '''fig.add_annotation(x='2017', y=600,\n",
    "                text=f\"MSE : {mse}\",\n",
    "                showarrow=False,\n",
    "                arrowhead=1)\n",
    "\n",
    "    fig.add_annotation(x='2017', y=550,\n",
    "                text=f\"MAE : {mae}\",\n",
    "                showarrow=False,\n",
    "                arrowhead=1)'''\n",
    "\n",
    "    fig.update_layout(hovermode='x',\n",
    "                     xaxis_title='Date',yaxis_title='Quantités')\n",
    "\n",
    "    #fig.show()\n",
    "    return(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7fa764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(residuals):\n",
    "    qqplot_data = qqplot(residuals['e'], line='s').gca().lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81288554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prophet_made_graph(forecast,m):\n",
    "    fig2 = m.plot(forecast)\n",
    "    a = add_changepoints_to_plot(fig2.gca(), m, forecast)\n",
    "    fig3 = m.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b82a67",
   "metadata": {},
   "source": [
    "# Entraîner le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0323f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#m, danone, future, forecast, residuals = train_neural_prophet_model(params)\n",
    "m, danone, future, forecast, residuals = train_prophet_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173c3e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prophet_made_graph(forecast,m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
